# ğŸˆ NFL Play Effectiveness & Win Probability Analytics Pipeline

## Overview

A real-time data engineering pipeline that ingests NFL play-by-play and player-tracking streams, processes them using Apache Spark on GCP, and produces win-probability and play-effectiveness metrics for coaches, analysts, and live broadcasts.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Play-by-Play Events (Pub/Sub)                                â”‚
â”‚  â€¢ Player Tracking Data (Pub/Sub)                               â”‚
â”‚  â€¢ Game Context (Cloud Storage)                                  â”‚
â”‚  â€¢ Historical Data (BigQuery)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STREAM PROCESSING (Dataproc/Spark)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Structured Streaming Jobs                                     â”‚
â”‚  â€¢ Real-time Joins (plays + tracking)                           â”‚
â”‚  â€¢ Data Quality Checks                                           â”‚
â”‚  â€¢ Feature Engineering                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA LAYERS (BigQuery)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Bronze  â†’ Raw events (plays, tracking)                         â”‚
â”‚  Silver  â†’ Cleaned & joined data                                â”‚
â”‚  Gold    â†’ Aggregated metrics (WPA, success rate)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVING LAYERS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Memorystore (Redis) - Real-time dashboards                   â”‚
â”‚  â€¢ BigQuery - Analytics & ML features                            â”‚
â”‚  â€¢ Cloud Storage - Data Lake                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CONSUMERS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Coaching Dashboards (Looker/Tableau)                         â”‚
â”‚  â€¢ ML Models (Vertex AI)                                         â”‚
â”‚  â€¢ Broadcast Graphics APIs                                       â”‚
â”‚  â€¢ Fan Apps                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tech Stack

- **Cloud Platform**: Google Cloud Platform (GCP)
- **Stream Processing**: Apache Spark (Dataproc), PySpark Structured Streaming
- **Orchestration**: Cloud Composer (Managed Airflow)
- **Message Queue**: Cloud Pub/Sub
- **Data Warehouse**: BigQuery
- **Caching**: Memorystore for Redis
- **Storage**: Cloud Storage (Data Lake)
- **Monitoring**: Cloud Monitoring, Cloud Logging
- **IaC**: Terraform
- **Language**: Python 3.11+

## Project Structure

```
nfl-play-analytics-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ nfl_streaming_pipeline_dag.py
â”‚   â”‚   â”œâ”€â”€ nfl_batch_pipeline_dag.py
â”‚   â”‚   â””â”€â”€ data_quality_dag.py
â”‚   â””â”€â”€ plugins/
â”‚       â””â”€â”€ custom_operators.py
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ play_by_play_processor.py
â”‚   â”‚   â”œâ”€â”€ player_tracking_processor.py
â”‚   â”‚   â””â”€â”€ stream_joiner.py
â”‚   â”œâ”€â”€ batch/
â”‚   â”‚   â”œâ”€â”€ historical_aggregator.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_quality.py
â”‚       â”œâ”€â”€ schema.py
â”‚       â””â”€â”€ transformations.py
â”‚
â”œâ”€â”€ data_generators/
â”‚   â”œâ”€â”€ play_event_simulator.py
â”‚   â”œâ”€â”€ tracking_data_simulator.py
â”‚   â””â”€â”€ game_context_generator.py
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dataproc_config.yaml
â”‚   â”œâ”€â”€ bigquery_schemas.json
â”‚   â””â”€â”€ pipeline_config.yaml
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ pubsub.tf
â”‚   â”œâ”€â”€ dataproc.tf
â”‚   â”œâ”€â”€ bigquery.tf
â”‚   â””â”€â”€ composer.tf
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”œâ”€â”€ gold_layer_views.sql
â”‚   â””â”€â”€ analytics_queries.sql
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”œâ”€â”€ test_data_quality.py
â”‚   â””â”€â”€ test_streaming.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## Core Metrics

### 1. Expected Yards Gained (EYG)
Predicted yards based on down, distance, field position, and historical data.

### 2. Play Success Rate
Percentage of plays that gain:
- 40%+ of yards needed on 1st down
- 60%+ of yards needed on 2nd down
- 100%+ of yards needed on 3rd/4th down

### 3. Win Probability Added (WPA)
Change in win probability before and after each play.

### 4. Defensive Pressure Rate
Frequency of defenders within 2 yards of QB at pass release.

### 5. Separation at Catch Point
Distance between receiver and nearest defender at catch.

## Setup Instructions

### Prerequisites

1. **GCP Account** with billing enabled
2. **GCP Project** created
3. **gcloud CLI** installed and configured
4. **Terraform** installed (v1.5+)
5. **Python** 3.11+

### Step 1: Clone Repository

```bash
git clone https://github.com/Teja9311/nfl-play-analytics-pipeline.git
cd nfl-play-analytics-pipeline
```

### Step 2: Set Environment Variables

```bash
export GCP_PROJECT_ID="your-project-id"
export GCP_REGION="us-central1"
export GCS_BUCKET="${GCP_PROJECT_ID}-nfl-data"
```

### Step 3: Infrastructure Deployment

```bash
cd terraform
terraform init
terraform plan -var="project_id=$GCP_PROJECT_ID" -var="region=$GCP_REGION"
terraform apply -var="project_id=$GCP_PROJECT_ID" -var="region=$GCP_REGION"
```

### Step 4: Install Python Dependencies

```bash
pip install -r requirements.txt
```

### Step 5: Deploy Airflow DAGs

```bash
# Get Composer environment details
gcloud composer environments describe nfl-composer-env \
    --location $GCP_REGION \
    --format="get(config.dagGcsPrefix)"

# Upload DAGs
gsutil -m cp -r airflow/dags/* gs://<composer-bucket>/dags/
```

### Step 6: Submit Spark Jobs

```bash
# Start streaming job
gcloud dataproc jobs submit pyspark \
    spark/streaming/play_by_play_processor.py \
    --cluster=nfl-dataproc-cluster \
    --region=$GCP_REGION \
    --jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar
```

### Step 7: Start Data Simulators (for testing)

```bash
python data_generators/play_event_simulator.py
python data_generators/tracking_data_simulator.py
```

## Data Flow

### Real-Time Path

1. **Ingestion**: Events published to Pub/Sub topics
2. **Stream Processing**: Spark Structured Streaming reads from Pub/Sub
3. **Transformation**: Clean, validate, join, enrich data
4. **Storage**: Write to BigQuery (bronze â†’ silver â†’ gold)
5. **Caching**: Push latest metrics to Redis
6. **Consumption**: APIs, dashboards, ML models

### Batch Path

1. **Ingestion**: Historical data loaded from Cloud Storage
2. **Processing**: Spark batch jobs aggregate metrics
3. **Feature Engineering**: Create ML features
4. **Storage**: Write to BigQuery gold layer
5. **Consumption**: Analytics, reporting, model training

## Key Features

### Data Quality
- Schema validation using Great Expectations
- Anomaly detection (impossible speeds, missing coordinates)
- Late data handling with watermarking
- Duplicate detection and deduplication

### Scalability
- Autoscaling Dataproc clusters
- Partitioned BigQuery tables (by game_id, date)
- Streaming inserts with batching
- Horizontal scaling for multiple concurrent games

### Monitoring
- Custom Cloud Monitoring dashboards
- Alerting on SLA violations
- Data freshness checks
- Pipeline performance metrics

## Sample Queries

### Win Probability by Play Type

```sql
SELECT 
  play_type,
  AVG(win_prob_added) as avg_wpa,
  COUNT(*) as play_count
FROM `project.nfl_gold.play_effectiveness`
WHERE season = 2025
  AND quarter <= 4
GROUP BY play_type
ORDER BY avg_wpa DESC;
```

### Top Pressure Situations

```sql
SELECT 
  game_id,
  play_id,
  qb_pressure_rate,
  sack_probability,
  play_result
FROM `project.nfl_silver.plays_with_tracking`
WHERE qb_pressure_rate > 0.6
ORDER BY qb_pressure_rate DESC
LIMIT 100;
```

## Performance Benchmarks

- **Latency**: < 2 seconds from event to dashboard
- **Throughput**: 10K events/sec per game
- **Concurrent Games**: Up to 16 games simultaneously
- **Data Freshness**: Real-time (< 1 second lag)

## Cost Optimization

- Use preemptible workers for Dataproc (60% cost savings)
- BigQuery partitioning and clustering
- Pub/Sub message retention = 7 days
- Autoscaling policies for compute resources

## Contributing

Pull requests welcome! Please follow:
1. Fork the repository
2. Create a feature branch
3. Write tests for new functionality
4. Submit PR with detailed description

